#!/usr/bin/env python3
"""Data analysis script for scraped YDL posts."""
import sys
import json
from pathlib import Path
from collections import Counter
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def load_dataset(dataset_path="data/dataset.jsonl"):
    """Load all posts from dataset."""
    posts = []
    
    dataset_file = Path(dataset_path)
    if not dataset_file.exists():
        print(f"âŒ Dataset not found: {dataset_path}")
        print("Run the scraper first: python main.py --mode full")
        return posts
    
    with open(dataset_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                posts.append(json.loads(line))
            except Exception as e:
                print(f"âš ï¸  Failed to parse line: {e}")
    
    return posts


def analyze_posts(posts):
    """Analyze post statistics."""
    if not posts:
        print("No posts to analyze")
        return
    
    print("=" * 70)
    print("ğŸ“Š å£¹ç‚¹çµæ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 70)
    print()
    
    # Basic stats
    print("ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡")
    print(f"  â€¢ æ€»å¸–å­æ•°: {len(posts)}")
    
    total_comments = sum(len(p.get('comments', [])) for p in posts)
    print(f"  â€¢ æ€»è¯„è®ºæ•°: {total_comments}")
    print(f"  â€¢ å¹³å‡è¯„è®ºæ•°: {total_comments / len(posts):.1f}")
    
    total_views = sum(p.get('view_count', 0) for p in posts)
    print(f"  â€¢ æ€»é˜…è¯»æ•°: {total_views:,}")
    print(f"  â€¢ å¹³å‡é˜…è¯»æ•°: {total_views / len(posts):.0f}")
    
    print()
    
    # User stats
    print("ğŸ‘¥ ç”¨æˆ·ç»Ÿè®¡")
    usernames = [p.get('username', 'æœªçŸ¥') for p in posts]
    user_counts = Counter(usernames)
    
    anonymous_count = sum(1 for p in posts if p.get('is_anonymous', False))
    print(f"  â€¢ ç‹¬ç«‹ç”¨æˆ·: {len(user_counts)}")
    print(f"  â€¢ åŒ¿åå¸–å­: {anonymous_count} ({anonymous_count/len(posts)*100:.1f}%)")
    
    print(f"\n  Top 5 æ´»è·ƒç”¨æˆ·:")
    for username, count in user_counts.most_common(5):
        print(f"    - {username}: {count} å¸–å­")
    
    print()
    
    # Topic stats
    print("ğŸ·ï¸  è¯é¢˜ç»Ÿè®¡")
    topics = [p.get('topic_title', '') for p in posts if p.get('topic_title')]
    
    if topics:
        topic_counts = Counter(topics)
        print(f"  â€¢ è¯é¢˜æ€»æ•°: {len(topic_counts)}")
        print(f"  â€¢ å¸¦è¯é¢˜å¸–å­: {len(topics)} ({len(topics)/len(posts)*100:.1f}%)")
        
        print(f"\n  çƒ­é—¨è¯é¢˜:")
        for topic, count in topic_counts.most_common(5):
            print(f"    - {topic}: {count} å¸–å­")
    else:
        print("  â€¢ æ— è¯é¢˜æ•°æ®")
    
    print()
    
    # Content stats
    print("ğŸ“ å†…å®¹ç»Ÿè®¡")
    content_lengths = [len(p.get('content', '')) for p in posts]
    avg_length = sum(content_lengths) / len(content_lengths)
    
    print(f"  â€¢ å¹³å‡å†…å®¹é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
    print(f"  â€¢ æœ€çŸ­å¸–å­: {min(content_lengths)} å­—ç¬¦")
    print(f"  â€¢ æœ€é•¿å¸–å­: {max(content_lengths)} å­—ç¬¦")
    
    print()
    
    # Engagement stats
    print("ğŸ’¬ äº’åŠ¨ç»Ÿè®¡")
    
    # Posts with most comments
    posts_by_comments = sorted(posts, key=lambda p: len(p.get('comments', [])), reverse=True)
    print(f"\n  è¯„è®ºæœ€å¤šçš„å¸–å­:")
    for post in posts_by_comments[:3]:
        content_preview = post.get('content', '')[:40] + '...'
        print(f"    - [{post.get('post_id')}] {content_preview}")
        print(f"      è¯„è®º: {len(post.get('comments', []))}, é˜…è¯»: {post.get('view_count', 0)}")
    
    # Posts with most views
    posts_by_views = sorted(posts, key=lambda p: p.get('view_count', 0), reverse=True)
    print(f"\n  é˜…è¯»æœ€å¤šçš„å¸–å­:")
    for post in posts_by_views[:3]:
        content_preview = post.get('content', '')[:40] + '...'
        print(f"    - [{post.get('post_id')}] {content_preview}")
        print(f"      é˜…è¯»: {post.get('view_count', 0)}, æ¸©æš–: {post.get('warm_count', 0)}")
    
    print()
    
    # Comment analysis
    if total_comments > 0:
        print("ğŸ’­ è¯„è®ºåˆ†æ")
        
        # User types in comments
        comment_user_types = []
        for post in posts:
            for comment in post.get('comments', []):
                user_type = comment.get('user_type')
                if user_type == 1:
                    comment_user_types.append('å’¨è¯¢å¸ˆ')
                else:
                    comment_user_types.append('æ™®é€šç”¨æˆ·')
        
        type_counts = Counter(comment_user_types)
        print(f"  â€¢ å’¨è¯¢å¸ˆè¯„è®º: {type_counts.get('å’¨è¯¢å¸ˆ', 0)}")
        print(f"  â€¢ æ™®é€šç”¨æˆ·è¯„è®º: {type_counts.get('æ™®é€šç”¨æˆ·', 0)}")
        
        # Reply types
        reply_types = []
        for post in posts:
            for comment in post.get('comments', []):
                reply_types.append(comment.get('reply_type', 'post'))
        
        reply_counts = Counter(reply_types)
        print(f"\n  â€¢ å›å¤ä¸»å¸–: {reply_counts.get('post', 0)}")
        print(f"  â€¢ å›å¤è¯„è®º: {reply_counts.get('comment', 0)}")
    
    print()
    print("=" * 70)
    print("âœ… åˆ†æå®Œæˆ")
    print("=" * 70)


def export_summary(posts, output_file="data/summary.json"):
    """Export summary statistics to JSON."""
    if not posts:
        return
    
    summary = {
        "generated_at": datetime.now().isoformat(),
        "total_posts": len(posts),
        "total_comments": sum(len(p.get('comments', [])) for p in posts),
        "total_views": sum(p.get('view_count', 0) for p in posts),
        "unique_users": len(set(p.get('username', '') for p in posts)),
        "anonymous_posts": sum(1 for p in posts if p.get('is_anonymous', False)),
        "topics": len(set(p.get('topic_title', '') for p in posts if p.get('topic_title')))
    }
    
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ Summary exported to: {output_path}")


def main():
    """Main analysis function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Analyze scraped YDL data")
    parser.add_argument(
        "--dataset",
        default="data/dataset.jsonl",
        help="Path to dataset file"
    )
    parser.add_argument(
        "--export",
        action="store_true",
        help="Export summary to JSON"
    )
    
    args = parser.parse_args()
    
    # Load data
    print("Loading dataset...")
    posts = load_dataset(args.dataset)
    
    if not posts:
        return
    
    print(f"âœ… Loaded {len(posts)} posts\n")
    
    # Analyze
    analyze_posts(posts)
    
    # Export summary
    if args.export:
        export_summary(posts)


if __name__ == "__main__":
    main()

